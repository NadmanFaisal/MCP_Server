

--- Page 1 ---

Engineering of ML Systems
Part 2
DIT826
Daniel Str√ºber

--- Page 2 ---

Learning objectives
‚Ä¢ Understand that building ML systems is more
than training a model
‚Ä¢ Understand practices and challenges of ML
systems engineering
2

--- Page 3 ---

The ML Workflow
‚Ä¢ Working with data:
‚Äì More issues
‚Äì Mitigations
3

--- Page 4 ---

4

--- Page 5 ---

SW engineering issues data
Unstable dependencies
‚á¢
‚Ä¢ SW design principle: ‚Äúminimize dependency to
unstable modules‚Äù
‚Ä¢ Unstable data dependencies
‚Äì Quality or quantity of data source deteriorates
‚Äì Format or schema of data sources changes
‚Ä¢ Especially problematic when models are
retrained frequently
5

--- Page 6 ---

SW engineering issues data
Unstable dependencies
‚á¢
‚Ä¢ Mitigation: Create versioned copy
(i.e., decoupling)
‚Ä¢ Mitigation: Monitor for upstream instability in
features
‚Äì What alert would fire if one datacenter stops sending
data?
‚Äì What if an upstream signal provider did a major
version upgrade?
6

--- Page 7 ---

CSV validator
Validation of CSV file
‚Ä¢ https://pypi.org/project/csvvalidator
# import everything from the csvvalidator package
from csvvalidator import *
# Specify which fields (columns) your .csv needs to have
# You should include all fields you use in your dashboard
field_names = ('date‚Äô,
'units_sold‚Äô,
'store‚Äô
)
# create a validator object using these fields
validator = CSVValidator(field_names)
7

--- Page 8 ---

CSV validator
# write some checks to make sure specific fields
# are the way we expect them to be
validator.add_value_check(
'date', # check for a date with the sepecified format
datetime_string('%Y-%m-%d'),
'EX1', # code for exception
'invalid date' # message to report if error thrown
)
validator.add_value_check(
'units_sold',
int,
'EX2',
'number of units sold not an integer'
)
validator.add_value_check(
'store',
enumeration('store1', 'store2'),
'EX4',
'store name not recognized‚Äô
)
problems = validator.validate(data) 8

--- Page 9 ---

SW engineering issues data
Unnecessary dependencies
‚á¢
‚Ä¢ SW design principle: ‚Äúkeep it simple‚Äù
‚Ä¢ Unnecessary data dependencies
‚Äì Correlated features
‚Äì -features (small improvement)
‚Äì Forgotten features (from previous experiment)*
ùúñùúñ
*Rule #22 ‚Äì Clean up features you are no longer using, in ‚ÄúM. Zinkevich, Rules of
Machine Learning‚Äù

--- Page 10 ---

SW engineering issues data
Unnecessary dependencies
‚á¢
‚Ä¢ Issue of cost
‚Äì Storage cost
(esp. if data is massively large)
(esp. if data is versioned)
‚Äì Computational cost
‚Äì Maintenance cost
‚Äì Monitoring cost

--- Page 11 ---

SW engineering issues data
Unnecessary dependencies
‚á¢
‚Ä¢ One Solution:
‚Äì Feature selection via exhaustive ‚Äòleave-one-
feature-out‚Äô evaluations.
F F
F F F F F F F
F F F F F F
Model Model Model
F F F F F F
F F F F F F F F F
Acc 90%
Acc 90% Acc 90%
11

--- Page 12 ---

SW engineering issues data
‚Ä¢ SW design principle: ‚Äúavoid module dependency
loops‚Äù ‚á¢
‚Ä¢ Hidden feedback loops
‚Äì Output of model A ‚Üí fed to model B
‚Äì Output of model B ‚Üí fed back to model A
‚Ä¢ Mitigation
‚Äì Reduce dependency
‚Äì Better understand the problem that you want to solve
‚Äì Testing
12

--- Page 13 ---

Questions?
13

--- Page 14 ---

The ML Workflow
‚Ä¢ Working with data:
‚Äì Feature Engineering
14

--- Page 15 ---

Feature Engineering
‚Ä¢ The art of converting context into features
that work well for your problem and with your
model structure.
Problem
Data + Context Features
15

--- Page 16 ---

Feature Engineering
‚Ä¢ Normalization and standardization: changing
numerical features so they are comparable
Standardized = Mean
ùíáùíá‚àí ùúáùúá
Std. Deviation
ùíáùíá
ùúéùúé
‚Ä¢ Expose hidden Information
‚Ä¢ Expand the context
16

--- Page 17 ---

Feature Engineering
Feature Selection
‚Ä¢ Which features to use?
‚Äì Use features that will help your model to generalize.
‚Ä¢ How many features to use?
‚Äì Approaches:
‚Ä¢ Frequency
‚Ä¢ Accuracy
‚Ä¢ ‚Ä¶
17

--- Page 18 ---

Feature Selection
Frequency
‚Ä¢ Pick top N most common features in the
training set.
Feature Count
to 1745
you 1526
Feature with a
I 1369
count > 1000
a 1337
the 1007
and 758
in 400
‚Ä¶ ‚Ä¶
18

--- Page 19 ---

Feature Selection
Accuracy
‚Ä¢ Pick N that improves the accuracy of the model.
‚Äì Maximize the accuracy gain from using these features.
‚Ä¢ Greedy search, adding/removing features into/from
the model:
‚Äì Add (remove) a candidate feature into/from the model.
‚Äì Build and train the model and evaluate it.
‚Äì Check the accuracy.
Remove Accuracy
‚Äì Add (remove) the feature.
<None> 88.2%
‚Äì Repeat until you get n.
to 92.5%
FREE 85.8%
‚Ä¶ ‚Ä¶

--- Page 20 ---

Questions?
20

--- Page 21 ---

The ML Workflow
‚Ä¢ Working with data:
‚Äì Model Training
21

--- Page 22 ---

A Challenge‚Ä¶
‚Ä¢ Different expertise is necessary
‚Äì Software development
‚Äì Data management (large data)
‚Äì Machine learning
‚Ä¢ Different teams might work in silos
‚Äì Data engineers: building data pipeline
‚Äì Data scientist: selecting and tuning the algorithm
‚Äì Web developers: building the client/server
22

--- Page 23 ---

‚Ä¶consequences
‚Ä¢ Models might only work in the lab
‚Äì Never leave the proof-of-concept state
‚Ä¢ Hard to update and maintain.
‚Ä¢ Many are advocating for CD practices applied
to production ML systems*
* D. Sato, A. Wider, C. Windheuser, Continuous delivery for machine learning,
https://martinfowler.com/articles/cd4ml.html
23

--- Page 24 ---

Static vs. dynamic training
Definition
‚Ä¢ Static training
‚Äì Inference model trained once on a fixed data set
‚Äì Put in production, does not change
‚Ä¢ Dynamic training
‚Äì Labeled data continuously coming into the system
‚Äì New data incorporated into the model
24

--- Page 25 ---

Static vs. dynamic training
When to use
‚Ä¢ Static training
‚Äì Modeled reality is not expected to change
(e.g., cat/dog pictures recognition, playing chess)
‚Ä¢ Dynamic training
‚Äì Modeled reality has trends and changes
(e.g., buying trends, recommendations, self-
driving cars, etc.)
25

--- Page 26 ---

Static vs. dynamic training
Attention points
‚Ä¢ Static training
‚Äì Model can degrade slowly.
‚Äì More effort on data collection and labeling
‚Ä¢ Dynamic training
‚Äì Need monitoring (going off the rail?)
‚Äì Need infrastructure for versioning and roll-back
‚Äì Less effort on data collection, but tricky to get it right.
26

--- Page 27 ---

Reproducibility and auditability
‚Ä¢ Keeping track of history is important
‚Ä¢ To have reproducible results, need to know
the exact version of
1. Source code
(e.g. model training and pre-processing)
2. Training data
(e.g. input signals and labels)
3. Platform
(e.g. OS, GPU, and version of installed packages)
27

--- Page 28 ---

Traceability and versioning
‚Ä¢ Data sets should be tagged with metadata
1. Origin
2. Freshness (when collected)
3. Code used to extract it
28

--- Page 29 ---

Traceability and versioning
‚Ä¢ Models should be tagged with provenance
‚Äì Which data used for training/testing?
‚Äì Which pipeline generated it?
‚Ä¢ Versioned copies should be kept of each
<code,data,model,environment> tuple
‚Ä¢ At least for models that make it to production
29

--- Page 30 ---

Versioning challenges
‚Ä¢ Versioning very large artifacts (e.g., data) is
difficult and costly.
30

--- Page 31 ---

Questions?
31

--- Page 32 ---

What is different with ML?
Traditional
A bit of this A lot of this
software project
ML
A lot of this An this A bit of this
software project
32

--- Page 33 ---

Code is code, right?
Design anti-patterns in ML systems
‚Ä¢ Pipeline jungles
‚Äì no modularization of pipeline code, no refactoring
‚Ä¢ Mitigation
‚Äì Modularize and organize the code.
‚Äì Testing.

--- Page 34 ---

Code is code, right?
Design anti-patterns in ML systems
‚Ä¢ Excessive glue code
‚Äì Massive amount of code written to get data
into/out of a general-purpose package
‚Ä¢ Mitigation
‚Äì Create clean native solutions where feasible
‚Äì Carefully consider the benefit and cost of adding
(yet another) general-purpose package

--- Page 35 ---

Code is code, right?
Design anti-patterns in ML systems
‚Ä¢ Dead experimental code paths
‚Äì no clean up discipline
‚Ä¢ Mitigation
‚Äì Periodical inspection of experimental code.

--- Page 36 ---

Example
Blob of code. What does it do?
df = pd.DataFrame(...)
del df['column1‚Äô]
df = df.dropna(subset=['column2', 'column3‚Äô])
df = df.rename({'column2': 'unicorns', 'column3': 'dragons‚Äô})
df['new_column'] = ['iterable', 'of', 'items‚Äô]
df.reset_index(inplace=True, drop=True)
36

--- Page 37 ---

Example
Blob of code. Explained!
# create a pandas DataFrame somehow
df = pd.DataFrame(...)
# delete a column from the dataframe
del df['column1‚Äô]
# drop rows that have empty values in column 2 and 3
df = df.dropna(subset=['column2', 'column3‚Äô])
# rename column2 and column3
df = df.rename({'column2': 'unicorns', 'column3': 'dragons‚Äô})
# add a new column
df['new_column'] = ['iterable', 'of', 'items‚Äô]
# reset index to account for the missing row we removed above
df.reset_index(inplace=True, drop=True)
Good comments explain rationale
37

--- Page 38 ---

Questions?
38

--- Page 39 ---

The ML Workflow
‚Ä¢ Working with data:
‚Äì Evaluation is creation
39

--- Page 40 ---

Evaluation
Machine Learning
Usually says
Output Output
ùë¶ùë¶ = 1
Process 1 Model 1 Process 2 Model 2
Data Data
Usually says 0
ùë¶ùë¶ =
‚Ä¢ Does Process 1 do a good job at mapping ‚Äòdata‚Äô to ‚Äòoutput‚Äô?
‚Ä¢ Is Model 2 better than Model 1?
‚Ä¢ Are the mistakes similar or different? Which is better?
40

--- Page 41 ---

The ML Workflow
‚Ä¢ Where intelligence lives
‚Ä¢ Intelligent experience
‚Ä¢ Related to software architecture ‚Äì
solutions are instances of architectural styles
41

--- Page 42 ---

Where Intelligence Lives
‚Ä¢ Static intelligence in the product
‚Ä¢ Client-side intelligence
‚Ä¢ Server-side intelligence
‚Ä¢ Back-end cached intelligence
42

--- Page 43 ---

Where Intelligence Lives
Static intelligence in the product
‚Ä¢ Model is packaged with the application
‚Ä¢ Pros
‚Äì Cost of operation: Cheap
‚Äì Latency in execution: Excellent
‚Äì Offline operation: Yes
‚Ä¢ Cons
‚Äì Latency in updating intelligence: Poor
‚Äì No data to improve the intelligence
43

--- Page 44 ---

Where Intelligence Lives
Client-side intelligence
‚Ä¢ Executes completely on the client.
‚Ä¢ Pros
‚Äì Latency in execution: Excellent
‚Äì Offline operation: Yes
‚Ä¢ It depends
‚Äì Latency in updating intelligence: Variable
‚Äì Cost of operation: Based on update rate
‚Ä¢ Cons
‚Äì Exposes intelligence to the world
44

--- Page 45 ---

Where Intelligence Lives
Server-centric intelligence
‚Ä¢ The intelligence runs in real-time in a service
‚Äì Client sends features to a server; the server executes the
intelligence on the features and returns the result.
‚Ä¢ Pros
‚Äì Latency in updating intelligence: Good
‚Äì Easier monitoring
‚Ä¢ It depends
‚Äì Latency in execution: Variable
‚Äì Cost of operation: Variable
‚Ä¢ Cons
‚Äì Offline operation: No
45

--- Page 46 ---

Where Intelligence Lives
Back-end cached intelligence
‚Ä¢ Involves running the intelligence off-line, caching the
results, and delivering these when needed.
‚Ä¢ Pros and Cons
‚Äì Latency in execution: Variable
‚Äì Latency in updating intelligence: Variable
‚Äì Cost of operation: Variable
‚Äì Offline operation: Partially
46

--- Page 47 ---

Intelligence Experience
‚Ä¢ A connection between the intelligence and
users.
‚Ä¢ Example: an intelligence determines that the
user is in a room where it is a little dark for
humans to see comfortably.
‚Äì How should the experience respond?
47

--- Page 48 ---

Intelligence Experience
Techniques
‚Ä¢ P (LikeSong |User, History)
‚Ä¢ Modes of Intelligent Interaction
Playing a Song‚Ä¶ Should I Play a Song? Choose a Song‚Ä¶ Choose a Song‚Ä¶
Yes No
Automate Prompt Organize Annotate
‚Ä¢ Which intelligence experience to use? When to
Update it?
48

--- Page 49 ---

Intelligence Experience
Goals
‚Ä¢ Achieve objectives & increase profit and sales
‚Ä¢ Engage users and achieve their objectives
‚Ä¢ Get data to improve
‚Ä¢
Example:
Home Light ÔÉ† P(on | sensors, motion, etc.)
‚Äì Objective?
P > 0.90; turn on light P < 0.25; turn light off
P <= 0.50 P > 0.50
(turn light off after time t)
P > 0.75; then prompt user
Full automation Save Energy User Safety 49

--- Page 50 ---

Questions?
50

--- Page 51 ---

The ML Workflow
‚Ä¢ Model Monitoring
51

--- Page 52 ---

Monitoring and Telemetry
‚Ä¢ Monitoring is foundational for producing
intelligence that:
‚Äì functions correctly, and
‚Äì improves over time.
‚Ä¢ It includes knowing:
‚Äì Context
‚Äì Answers
‚Äì Experiences
‚Äì User behavior
‚Ä¢ Telemetry: collect data at remote points and
transmit them to a central point for monitoring
52

--- Page 53 ---

Monitoring inference output
‚Ä¢ Model output
‚Äì Predicted value is within bounds
‚Äì Warning sign of model going off the rail
‚Ä¢ Add sanity checks on output
‚Ä¢ Examples
‚Äì Predicted house not 10x bigger in same area
‚Äì Values are within specified distributions
53

--- Page 54 ---

Monitoring: Rolling back
‚Ä¢ Model ‚Äúroll back‚Äù procedure: Being able to
quickly revert to a previous known-good model
‚Ä¢ Test how quickly and safely a model can be
rolled back.

--- Page 55 ---

Intelligence Orchestration
‚Ä¢ Achieve objectives of the ML system
‚Ä¢ Controls the monitoring
‚Ä¢ Balance experience
‚Ä¢ Deals with mistakes
55

--- Page 56 ---

Orchestrating intelligent systems
Why needed?
‚Ä¢ Objective changes: ‚Ä¢ Intelligence changes:
‚Äì Better understand a ‚Äì More data, better
problem models.
‚Äì Solved previous objective ‚Äì More accurate or less
accurate model.
‚Ä¢ Cost changes:
‚Ä¢ User changes:
‚Äì Telemetry costs
‚Äì New users or users leave
‚Äì Mistakes costs
56

--- Page 57 ---

Questions?
57

--- Page 58 ---

Summary: lifecycle of data-intensive AI systems
Prepare data
‚Ä¢ Ingestion, Labeling, Normalization,
Transformation, Validation
Source: D. Sato, A. Wider, C. Windheuser, Continuous Delivery for Machine Learning
https://martinfowler.com/articles/cd4ml.html 58

--- Page 59 ---

Lifecycle of data-intensive AI systems
Experiment and build model
‚Ä¢ Select features, Select ML algorithm, Tune
parameters, perform evaluation and testing
Source: D. Sato, A. Wider, C. Windheuser, Continuous Delivery for Machine Learning 59

--- Page 60 ---

Lifecycle of data-intensive AI systems
Serving the model in production
‚Ä¢ Deploy, Predict, Monitor, Update model
Source: D. Sato, A. Wider, C. Windheuser, Continuous Delivery for Machine Learning 60

--- Page 61 ---

Questions?
61